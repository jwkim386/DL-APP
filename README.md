# 2022-1 Deep Learning and Applications

In this lecture, we will be learning about two different topics in deep learning: self-supervised learning (SSL) and generative models. 

### Syllabus
- Week 1 (3/7): Historical Review
- Week 2 (3/15): Good Old Fashioned SSL (`Jigsaw`, `BiGAN`, `RotNet`, `Auto-Encoding Transform`, `DeepCluster`, `Single Image SSL`)
- Week 3 (3/22): Convnet-based SSL (`DrLIM`, `Contrastive Predictive Coding`, `SimCLR`, `MoCo`, `BYOL`, `SimCLRv2`, `SwAV`, `Barlow Twins`)
- Week 4 (3/29): Transformer-based SSL (`Transformer`, `ViT`, `Swin Transformer`, `DINO`, `EsViT`)
- Week 5 (4/5): Language-domain SSL (`GPT`, `GPT-2`, `BERT`, `RoBERTa`, `ALBERT`, `GPT-3`)
- Week 6:(4/12): Invited Talk ([Sangdoo Yun](https://sangdooyun.github.io/)@Naver AI Lab)
- Week 7 (4/19): Interlim Presentation 1
- Week 8 (4/26): Interlim Presentation 2
- Week 9 (5/3): Invited Talk ([Minsuk Chang](https://minsukchang.com/)@Naver AI Lab)
- Week 10 (5/10): Generative Model 1 (AR model, ML learning)
- Week 11 (5/17): Generative Model 2 (VAE, WAE, GAN, Flow-based models)
- Week 12 (5/24): Generative Model 3 (DDPM)
- Week 13 (5/31): Generative Model 4 (More diffusion-based + score-based models)
- Week 14: Final Presentations 1
- Week 15: Final Presentations 2
- Week 16: Final Presentations 3

### Paper Lists
- `Jigsaw`: "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles," 2017
- `BiGAN`: "ADVERSARIAL FEATURE LEARNING," 2017
- `RotNet`: "UNSUPERVISED REPRESENTATION LEARNING BY PREDICTING IMAGE ROTATIONS," 2018
- `Auto-Encoding Transform`: "AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data," 2019
- `DeepCluster`: "Deep Clustering for Unsupervised Learning of Visual Features," 2019
- `Single Image SSL`: "A CRITICAL ANALYSIS OF SELF-SUPERVISION, WHAT WE CAN LEARN FROM A SINGLE IMAGE," 2020
- `DrLIM`: "Dimensionality Reduction by Learning an Invariant Mapping," 2006
- `Contrastive Predictive Coding`: "Representation Learning with Contrastive Predictive Coding," 2019
- `SimCLR`: "A Simple Framework for Contrastive Learning of Visual Representations," 2020
- `MoCo`: "Momentum Contrast for Unsupervised Visual Representation Learning," 2020
- `BYOL`: "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning," 2020
- `SimCLRv2`: "Big Self-Supervised Models are Strong Semi-Supervised Learners," 2020
- `SwAV`: "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments," 2021
- `Barlow Twins`: "Barlow Twins: Self-Supervised Learning via Redundancy Reduction," 2021
- `Transformer`: "Attention is All You Need," 2017
- `ViT`: "AN IMAGE IS WORTH 16 X 16 WORDS :TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE," 2021
- `Swin Transformer`: "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows," 2021
- `DINO`: "Emerging Properties in Self-Supervised Vision Transformers," 2021
- `EsViT`: "Efficient Self-supervised Vision Transformers for Representation Learning," 2021
- `GPT`: "Improving Language Understanding by Generative Pre-Training," 2018
- `GPT-2`: "Language Models are Unsupervised Multitask Learners," 2018
- `BERT`: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding," 2019
- `RoBERTa`: "RoBERTa: A Robustly Optimized BERT Pretraining Approach," 2019
- `ALBERT`: "ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS," 2020
- `GPT-3`: "Language Models are Few-Shot Learners," 2020

##### This syllabus is subject to further change or revision, as needed, to best realize the educational goals of the course.
