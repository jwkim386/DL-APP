# 2022-1 Deep Learning and Applications

In this lecture, we will be learning about two different topics in deep learning: self-supervised learning (SSL) and generative models. 

### Syllabus
- Week 1 (3/7): Historical Review
- Week 2 (3/15): Good Old Fashioned SSL (`Jigsaw`, `BiGAN`, `RotNet`, `Auto-Encoding Transform`, `DeepCluster`, `Single Image SSL`)
- Week 3 (3/22): Convnet-based SSL (`DrLIM`, `Contrastive Predictive Coding`, `SimCLR`, `MoCo`, `BYOL`, `SimCLRv2`, `SwAV`, `Barlow Twins`)
- Week 4 (3/29): Transformer-based SSL (`Transformer`, `ViT`, `Swin Transformer`, `DINO`, `EsViT`)
- Week 5 (4/5): Language-domain SSL (`GPT`, `GPT-2`, `BERT`, `RoBERTa`, `ALBERT`, `GPT-3`)
- Week 6:(4/12): Invited Talk ([Sangdoo Yun](https://sangdooyun.github.io/)@Naver AI Lab)
- Week 7 (4/19): Interlim Presentation 1
- Week 8 (4/26): Interlim Presentation 2
- Week 9 (5/10): Generative Model 1 (`NADE`,`PixelRNN`,`PixelCNN`)
- Week 10 (5/17): Generative Model 2 (`VAE`, `WAE`, `GAN`, `PlanarFlow`)
- Week 11 (5/24): Generative Model 3 (`DDPM`)
- Week 12 (5/31): Generative Model 4 (More diffusion-based models)
- Week 13 (6/7): Final Presentations 1
- Week 14 (6/14): Invited Talk ([Geonho Cha](https://www.linkedin.com/in/geonho-cha-37b033136/?originalSubdomain=kr)@Naver Clova)
- Week 15 (6/21): Final Presentations 2

### Paper Lists
- `Jigsaw`: "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles," 2017
- `BiGAN`: "ADVERSARIAL FEATURE LEARNING," 2017
- `RotNet`: "UNSUPERVISED REPRESENTATION LEARNING BY PREDICTING IMAGE ROTATIONS," 2018
- `Auto-Encoding Transform`: "AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data," 2019
- `DeepCluster`: "Deep Clustering for Unsupervised Learning of Visual Features," 2019
- `Single Image SSL`: "A CRITICAL ANALYSIS OF SELF-SUPERVISION, WHAT WE CAN LEARN FROM A SINGLE IMAGE," 2020
- `DrLIM`: "Dimensionality Reduction by Learning an Invariant Mapping," 2006
- `Contrastive Predictive Coding`: "Representation Learning with Contrastive Predictive Coding," 2019
- `SimCLR`: "A Simple Framework for Contrastive Learning of Visual Representations," 2020
- `MoCo`: "Momentum Contrast for Unsupervised Visual Representation Learning," 2020
- `BYOL`: "Bootstrap Your Own Latent A New Approach to Self-Supervised Learning," 2020
- `SimCLRv2`: "Big Self-Supervised Models are Strong Semi-Supervised Learners," 2020
- `SwAV`: "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments," 2021
- `Barlow Twins`: "Barlow Twins: Self-Supervised Learning via Redundancy Reduction," 2021
- `Transformer`: "Attention is All You Need," 2017
- `ViT`: "AN IMAGE IS WORTH 16 X 16 WORDS :TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE," 2021
- `Swin Transformer`: "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows," 2021
- `DINO`: "Emerging Properties in Self-Supervised Vision Transformers," 2021
- `EsViT`: "Efficient Self-supervised Vision Transformers for Representation Learning," 2021
- `GPT`: "Improving Language Understanding by Generative Pre-Training," 2018
- `GPT-2`: "Language Models are Unsupervised Multitask Learners," 2018
- `BERT`: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding," 2019
- `RoBERTa`: "RoBERTa: A Robustly Optimized BERT Pretraining Approach," 2019
- `ALBERT`: "ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS," 2020
- `GPT-3`: "Language Models are Few-Shot Learners," 2020
- `NADE`: "Neural Autoregressive Distribution Estimation." 2016
- `PixelRNN`: "Pixel Recurrent Neural Networks," 2016
- `PixelCNN`: "Conditional Image Generation with PixelCNN Decoders," 2016
- `VAE`: "Auto-Encoding Variational Bayes," 2013
- `WAE`: "Wasserstein Auto-Encoders," 2017
- `GAN`: "Generative Adversarial Networks," 2014
- `PlanarFlow`: "Variational Inference with Normalizing Flows," 2016
- `DDPM`: "Denoising Diffusion Probabilistic Models," 2020

##### This syllabus is subject to further change or revision, as needed, to best realize the educational goals of the course.
